{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_DataGathering",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UY5CIej-lu8",
        "colab_type": "text"
      },
      "source": [
        "### Gathering Data from Web\n",
        "\n",
        "Idea is to use python to extract the data from Web and use it for analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jzz4f0N-r1U",
        "colab_type": "code",
        "outputId": "7cf5423c-f008-4c58-b984-f544cc1500ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpIc1LlxXd30",
        "colab_type": "code",
        "outputId": "7a85049a-c754-454d-9469-52a629039c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls /content/drive/My\\ Drive/Datahack\\ NLP\\ Workshop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " big.txt\t    Disaster   HelperCodes    Water\n",
            "'Colab Notebooks'   GMB_NER    HotelReviews\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlO1BHbB-J0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import requests\n",
        "from datetime import timedelta, date"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svBzFEmq_nCC",
        "colab_type": "text"
      },
      "source": [
        "### Scrape the data from URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZpuXq_t-QD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def daterange(date1, date2):\n",
        "    for n in range(int ((date2 - date1).days)+1):\n",
        "        yield date1 + timedelta(n)\n",
        "\n",
        "url = \"http://123.63.203.150/reserve.asp\"\n",
        "data_path = \"/content/drive/My Drive/Datahack NLP Workshop/Water/\"\n",
        "start_dt = date(2016, 1, 1)\n",
        "end_dt = date(2016, 1, 3) \n",
        "\n",
        "for dt in daterange(start_dt, end_dt):\n",
        "    # post the information and get the html response\n",
        "    ldate = str(dt.strftime(\"%d/%m/%Y\"))\n",
        "    data = {\"ldate\":ldate, \"b1\":\"submit\"}\n",
        "    html_response = requests.post(url, data)\n",
        "    \n",
        "    # save it to file\n",
        "    fname = str(dt.strftime(\"%d%m%Y\")) + \".html\"\n",
        "    with open(data_path+fname, \"wb\") as f:\n",
        "        f.write(html_response.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq8WuYLC_8ls",
        "colab_type": "text"
      },
      "source": [
        "### Parse the HTML file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "028EI-ww_DQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import time\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import timedelta, date"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09QsoshAEWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def daterange(date1, date2):\n",
        "    for n in range(int ((date2 - date1).days)+1):\n",
        "        yield date1 + timedelta(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxaSPQy1AGuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Get Water Level Data\n",
        "df = []\n",
        "for dt in daterange(start_dt, end_dt):\n",
        "        fname = data_path + str(dt.strftime(\"%d%m%Y\")) + \".html\"\n",
        "        with open(fname, \"rb\") as fhandle:\n",
        "                html_doc = fhandle.read()\n",
        "\n",
        "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "        text_list = []\n",
        "        for td in soup.find_all(\"td\"):\n",
        "                text = \" \".join(td.get_text().strip().split())\n",
        "                text_list.append(text)\n",
        "        text_list = text_list[:-3]\n",
        "\n",
        "        try:\n",
        "                text_list = np.array(text_list).reshape([10,9])\n",
        "        except:\n",
        "                print(fname)\n",
        "                print(text_list)\n",
        "                print(prev_text_list)\n",
        "                print()\n",
        "                text_list = prev_text_list\n",
        "\n",
        "        olist = [str(dt.strftime(\"%d-%m-%Y\")), text_list[1,4], text_list[2,4], text_list[3,4], text_list[5,4]]\n",
        "        prev_text_list = text_list[:]\n",
        "        df.append(olist)\n",
        "\n",
        "df = pd.DataFrame(df, columns=[\"Date\", \"POONDI\", \"CHOLAVARAM\", \"REDHILLS\", \"CHEMBARAMBAKKAM\"])\n",
        "df.to_csv(data_path + \"chennai_reservoir_levels.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}