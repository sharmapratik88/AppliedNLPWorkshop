{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_TextClassification_BERT",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiveQKy_KNv3",
        "colab_type": "text"
      },
      "source": [
        "### Text Classification with BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ie7ucS4MCsn",
        "colab_type": "code",
        "outputId": "abcf15ca-661e-4ce3-d98a-7d0d2c45c956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_LyAutI6w-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvKQXk7D7HBX",
        "colab_type": "code",
        "outputId": "f86e7de9-dcb9-497f-a8b8-66751df82efd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\r\u001b[K     |█                               | 10kB 35.6MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 8.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 9.1MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 74.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 74.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 71.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=d3547c676c0b3fb715d8160f71060629530bb09f4108c0176536128a23b9dbf0\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: regex, sentencepiece, sacremoses, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhlQ7s8B7CTd",
        "colab_type": "code",
        "outputId": "3afdfe2b-8002-4a25-921c-1cba74e28f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "from transformers import (WEIGHTS_NAME, \n",
        "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
        "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer, \n",
        "                          XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n",
        "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
        "from transformers import AdamW, WarmupLinearSchedule\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZCisUd77Ro8",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r6eVvBL7FCJ",
        "colab_type": "code",
        "outputId": "6130a255-2df9-439f-8453-e26ac99bd397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "data_path = \"/content/drive/My Drive/Datahack NLP Workshop/Disaster/\"\n",
        "df = pd.read_csv(data_path + \"socialmedia_disaster_tweets.csv\", encoding='iso-8859-1')\n",
        "df = df[[\"choose_one\", \"text\"]]\n",
        "df.columns = [\"label\", \"text\"]\n",
        "df = df[df[\"label\"].isin([\"Relevant\", \"Not Relevant\"])].reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text\n",
              "0  Relevant                 Just happened a terrible car crash\n",
              "1  Relevant  Our Deeds are the Reason of this #earthquake M...\n",
              "2  Relevant  Heard about #earthquake is different cities, s...\n",
              "3  Relevant  there is a forest fire at spot pond, geese are...\n",
              "4  Relevant             Forest fire near La Ronge Sask. Canada"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uRGk4HF7Ul9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_map = {\"Relevant\":1, \"Not Relevant\":0}\n",
        "df[\"label\"] = df[\"label\"].map(label_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EXNGzDXGUJ0",
        "colab_type": "code",
        "outputId": "d22b58ac-c651-4e49-9a86-59e73be1dd57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_df, test_df = model_selection.train_test_split(df, test_size=0.2, random_state=2019)\n",
        "print(\"Train data shape is : \",train_df.shape)\n",
        "print(\"Test data shape is : \",test_df.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape is :  (8688, 2)\n",
            "Test data shape is :  (2172, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VKj21ehFEAr",
        "colab_type": "text"
      },
      "source": [
        "### Custom Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvrM3hDv7khK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_text_to_features(examples, tokenizer,\n",
        "                                      max_length=512,\n",
        "                                      pad_on_left=False,\n",
        "                                      pad_token=0,\n",
        "                                      pad_token_segment_id=0,\n",
        "                                      mask_padding_with_zero=True):\n",
        "    \"\"\"\n",
        "    Loads a data file into a list of ``InputFeatures``\n",
        "    Args:\n",
        "        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
        "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
        "        max_length: Maximum example length\n",
        "        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n",
        "        pad_token: Padding token\n",
        "        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n",
        "        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n",
        "            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n",
        "            actual values)\n",
        "    Returns:\n",
        "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
        "        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n",
        "        a list of task-specific ``InputFeatures`` which can be fed to the model.\n",
        "    \"\"\"\n",
        "    features = [[],[],[]]\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            print(\"Writing example %d\" % (ex_index))\n",
        "\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            example,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_length - len(input_ids)\n",
        "        if pad_on_left:\n",
        "            input_ids = ([pad_token] * padding_length) + input_ids\n",
        "            attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n",
        "            token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n",
        "        else:\n",
        "            input_ids = input_ids + ([pad_token] * padding_length)\n",
        "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
        "        assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask), max_length)\n",
        "        assert len(token_type_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_type_ids), max_length)\n",
        "\n",
        "        # if ex_index < 1:\n",
        "        #     print(\"*** Example ***\")\n",
        "        #     print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "        #     print(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "        #     print(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
        "\n",
        "        features[0].append(input_ids)\n",
        "        features[1].append(attention_mask)\n",
        "        features[2].append(token_type_ids)\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuU5_-qsFH96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed=123):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RoqRPd2FMVq",
        "colab_type": "code",
        "outputId": "c9e543e6-683c-4613-8809-bd11c34341c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDi4uFU_FRNc",
        "colab_type": "text"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FPAQvO3JhCY",
        "colab_type": "text"
      },
      "source": [
        "##### Model Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbYhKOgsFPjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
        "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
        "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etK-wUQtJjYA",
        "colab_type": "text"
      },
      "source": [
        "##### Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY00xwV5FZnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"bert\"\n",
        "pretrained_model_name = \"bert-base-uncased\"\n",
        "n_classes = 1\n",
        "max_length = 128\n",
        "batch_size = 8\n",
        "n_epochs = 1\n",
        "accumulation_steps = 1\n",
        "lr = 2e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG2DIzqyFb7c",
        "colab_type": "code",
        "outputId": "d2fdb9c8-a833-4a24-af44-1b400223d664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[model_name]\n",
        "config = config_class.from_pretrained(pretrained_model_name)\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_model_name, do_lower_case=True)\n",
        "model = model_class.from_pretrained(pretrained_model_name, num_labels=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 108631.95B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 897472.45B/s]\n",
            "100%|██████████| 440473133/440473133 [00:16<00:00, 26619937.36B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIU9YzKnJpHD",
        "colab_type": "text"
      },
      "source": [
        "##### Train Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH_VtYKNGiZU",
        "colab_type": "code",
        "outputId": "b664cdce-8a11-4add-f7f5-2c7b74287d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "train_df[\"text\"] = train_df[\"text\"].astype(str).fillna(\"NA\")\n",
        "train_features = convert_text_to_features(train_df[\"text\"], tokenizer, max_length=max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing example 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RncCyZGqGjVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.tensor(train_features[0], dtype=torch.long)\n",
        "X_mask = torch.tensor(train_features[1], dtype=torch.long)\n",
        "X_seg_ids = torch.tensor(train_features[2], dtype=torch.long)\n",
        "y = train_df[\"label\"].values\n",
        "y = torch.tensor(y[:,np.newaxis], dtype=torch.float32)\n",
        "\n",
        "train_dataset = data.TensorDataset(X, X_mask, X_seg_ids, y)\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_t7oIURJt7A",
        "colab_type": "text"
      },
      "source": [
        "##### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmN-rKcUGzwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "num_train_optimization_steps = int(n_epochs*len(train_dataset)/batch_size/accumulation_steps)\n",
        "num_warmup_steps = int(0.05*num_train_optimization_steps)\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n",
        "scheduler = WarmupLinearSchedule(optimizer,\n",
        "                                 warmup_steps=num_warmup_steps,\n",
        "                                 t_total=num_train_optimization_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMSlfPxzJw8v",
        "colab_type": "text"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhfUvF0OG3u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_everything()\n",
        "model.to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for x_batch, x_mask, x_seg_ids, y_batch in train_loader:\n",
        "        outputs = model(x_batch.to(device),\n",
        "                        attention_mask=x_mask.to(device),\n",
        "                        token_type_ids=x_seg_ids.to(device),\n",
        "                        labels=None)\n",
        "        y_pred = outputs[0]\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HJ3iX4tJ1GG",
        "colab_type": "text"
      },
      "source": [
        "##### Test Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6suy7C9G7Rd",
        "colab_type": "code",
        "outputId": "f1acc77c-af44-4f0f-8a96-99dbf6df2754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "test_df[\"text\"] = test_df[\"text\"].astype(str).fillna(\"NA\")\n",
        "test_features = convert_text_to_features(test_df[\"text\"], tokenizer, max_length=max_length)\n",
        "\n",
        "test_X = torch.tensor(test_features[0], dtype=torch.long)\n",
        "test_X_mask = torch.tensor(test_features[1], dtype=torch.long)\n",
        "test_X_seg_ids = torch.tensor(test_features[2], dtype=torch.long)\n",
        "test_y = test_df[\"label\"].values\n",
        "\n",
        "test_dataset = data.TensorDataset(test_X, test_X_mask, test_X_seg_ids)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing example 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWAfpZQ1J8AX",
        "colab_type": "text"
      },
      "source": [
        "##### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmOcj28NHifi",
        "colab_type": "code",
        "outputId": "967547cf-e077-4f82-f89b-03b0d097764d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "preds = np.zeros([len(test_dataset), 1])\n",
        "model.eval()\n",
        "for i, (x_batch, x_mask, x_seg_ids) in enumerate(test_loader):\n",
        "    outputs = model(x_batch.to(device),\n",
        "                    attention_mask=x_mask.to(device),\n",
        "                    token_type_ids=x_seg_ids.to(device),\n",
        "                    labels=None)\n",
        "    y_pred = sigmoid(outputs[0].detach().cpu().numpy())\n",
        "    preds[i*batch_size:(i+1)*batch_size, :] = y_pred\n",
        "    \n",
        "from sklearn import metrics\n",
        "metrics.roc_auc_score(test_y, preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8876832649879807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9VP7d7zJ_04",
        "colab_type": "text"
      },
      "source": [
        "### DIY - Build BERT cased model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41pKwgcZJBdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"bert\"\n",
        "pretrained_model_name = \"bert-base-cased\"\n",
        "n_classes = 1\n",
        "max_length = 128\n",
        "batch_size = 8\n",
        "n_epochs = 1\n",
        "accumulation_steps = 1\n",
        "lr = 2e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QToJFNFOwgSk",
        "colab_type": "code",
        "outputId": "bc2cfada-f0a8-4f39-ef23-ca6dce25775d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[model_name]\n",
        "config = config_class.from_pretrained(pretrained_model_name)\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_model_name, do_lower_case=True)\n",
        "model = model_class.from_pretrained(pretrained_model_name, num_labels=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 83411.73B/s]\n",
            "100%|██████████| 213450/213450 [00:00<00:00, 841004.19B/s]\n",
            "100%|██████████| 435779157/435779157 [00:17<00:00, 25391488.98B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BdzA1-ewi_f",
        "colab_type": "code",
        "outputId": "0e75312a-d5f2-436e-890a-37f0157a0fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "train_df[\"text\"] = train_df[\"text\"].astype(str).fillna(\"NA\")\n",
        "train_features = convert_text_to_features(train_df[\"text\"], tokenizer, max_length=max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing example 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBpc1jN_wmPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.tensor(train_features[0], dtype=torch.long)\n",
        "X_mask = torch.tensor(train_features[1], dtype=torch.long)\n",
        "X_seg_ids = torch.tensor(train_features[2], dtype=torch.long)\n",
        "y = train_df[\"label\"].values\n",
        "y = torch.tensor(y[:,np.newaxis], dtype=torch.float32)\n",
        "\n",
        "train_dataset = data.TensorDataset(X, X_mask, X_seg_ids, y)\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iProcYcTwpEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "num_train_optimization_steps = int(n_epochs*len(train_dataset)/batch_size/accumulation_steps)\n",
        "num_warmup_steps = int(0.05*num_train_optimization_steps)\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n",
        "scheduler = WarmupLinearSchedule(optimizer,\n",
        "                                 warmup_steps=num_warmup_steps,\n",
        "                                 t_total=num_train_optimization_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIINGII2wsHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_everything()\n",
        "model.to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for x_batch, x_mask, x_seg_ids, y_batch in train_loader:\n",
        "        outputs = model(x_batch.to(device),\n",
        "                        attention_mask=x_mask.to(device),\n",
        "                        token_type_ids=x_seg_ids.to(device),\n",
        "                        labels=None)\n",
        "        y_pred = outputs[0]\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OzmKjTkwvYo",
        "colab_type": "code",
        "outputId": "4e263303-7991-4e36-a968-7ddeeb7483b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "test_df[\"text\"] = test_df[\"text\"].astype(str).fillna(\"NA\")\n",
        "test_features = convert_text_to_features(test_df[\"text\"], tokenizer, max_length=max_length)\n",
        "\n",
        "test_X = torch.tensor(test_features[0], dtype=torch.long)\n",
        "test_X_mask = torch.tensor(test_features[1], dtype=torch.long)\n",
        "test_X_seg_ids = torch.tensor(test_features[2], dtype=torch.long)\n",
        "test_y = test_df[\"label\"].values\n",
        "\n",
        "test_dataset = data.TensorDataset(test_X, test_X_mask, test_X_seg_ids)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing example 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsN4pRtfwyGk",
        "colab_type": "code",
        "outputId": "d38ef5ad-ab30-4556-c80b-186cb8dfe90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "preds = np.zeros([len(test_dataset), 1])\n",
        "model.eval()\n",
        "for i, (x_batch, x_mask, x_seg_ids) in enumerate(test_loader):\n",
        "    outputs = model(x_batch.to(device),\n",
        "                    attention_mask=x_mask.to(device),\n",
        "                    token_type_ids=x_seg_ids.to(device),\n",
        "                    labels=None)\n",
        "    y_pred = sigmoid(outputs[0].detach().cpu().numpy())\n",
        "    preds[i*batch_size:(i+1)*batch_size, :] = y_pred\n",
        "    \n",
        "from sklearn import metrics\n",
        "metrics.roc_auc_score(test_y, preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8856596332458466"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}